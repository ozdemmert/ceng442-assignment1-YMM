{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38dba363",
   "metadata": {},
   "source": [
    "\n",
    "# CENG 442 ‚Äî Azerbaijani Text Preprocessing & Word Embeddings (Domain-Aware)\n",
    "\n",
    "This notebook builds a complete, reproducible pipeline for the assignment:\n",
    "\n",
    "**Outputs**\n",
    "- Five 2‚Äëcolumn Excel files: `cleaned_text`, `sentiment_value` (0.0 / 0.5 / 1.0)\n",
    "- One combined, domain‚Äëtagged corpus: `corpus_all.txt` (one sentence per line)\n",
    "- Two embedding models trained on the combined corpus: `embeddings/word2vec.model`, `embeddings/fasttext.model`\n",
    "- Simple comparison of Word2Vec vs FastText (coverage, synonym/antonym similarity, nearest neighbors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a936ae7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: c:\\Users\\merte\\OneDrive\\Masa√ºst√º\\CENG442 nlp\\nlp-hw1-main\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import re, html, unicodedata\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Where to read/write\n",
    "DATA_DIR = Path('Excels')\n",
    "OUT_DIR = Path('outputs')\n",
    "EMB_DIR = Path('embeddings')\n",
    "EMB_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print('Working directory:', Path.cwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2218285d",
   "metadata": {},
   "source": [
    "## Normalization utilities (Azerbaijani-aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a9427fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Azerbaijani-aware lowercase\n",
    "def lower_az(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = unicodedata.normalize(\"NFC\", s)\n",
    "    s = s.replace(\"I\", \"ƒ±\").replace(\"ƒ∞\", \"i\")\n",
    "    s = s.lower().replace(\"i\",\"i\")\n",
    "    return s\n",
    "\n",
    "HTML_TAG_RE = re.compile(r\"<[^>]+>\")\n",
    "URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+)\", re.IGNORECASE)\n",
    "EMAIL_RE = re.compile(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", re.IGNORECASE)\n",
    "PHONE_RE = re.compile(r\"\\+?\\d[\\d\\-\\s\\(\\)]{6,}\\d\")\n",
    "USER_RE = re.compile(r\"@\\w+\")\n",
    "MULTI_PUNCT = re.compile(r\"([!?.,;:])\\1{1,}\")\n",
    "MULTI_SPACE = re.compile(r\"\\s+\")\n",
    "REPEAT_CHARS = re.compile(r\"(.)\\1{2,}\", flags=re.UNICODE)\n",
    "\n",
    "TOKEN_RE = re.compile(\n",
    "    r\"[A-Za-z∆è…ôƒûƒüIƒ±ƒ∞i√ñ√∂√ú√º√á√ß≈û≈üXxQq]+(?:'[A-Za-z∆è…ôƒûƒüIƒ±ƒ∞i√ñ√∂√ú√º√á√ß≈û≈üXxQq]+)?\"\n",
    "    r\"|<NUM>|URL|EMAIL|PHONE|USER|EMO_(?:POS|NEG)\"\n",
    ")\n",
    "\n",
    "# Tiny emoji map\n",
    "EMO_MAP = {\"üôÇ\":\"EMO_POS\",\"üòÄ\":\"EMO_POS\",\"üòç\":\"EMO_POS\",\"üòä\":\"EMO_POS\",\"üëç\":\"EMO_POS\",\n",
    "           \"‚òπ\":\"EMO_NEG\",\"üôÅ\":\"EMO_NEG\",\"üò†\":\"EMO_NEG\",\"üò°\":\"EMO_NEG\",\"üëé\":\"EMO_NEG\"}\n",
    "\n",
    "# Light slang / deasciify map\n",
    "SLANG_MAP = {\"slm\":\"salam\",\"tmm\":\"tamam\",\"sagol\":\"saƒüol\",\"cox\":\"√ßox\",\"yaxsi\":\"yax≈üƒ±\"}\n",
    "NEGATORS  = {\"yox\",\"deyil\",\"he√ß\",\"q…ôtiyy…ôn\",\"yoxdur\"}\n",
    "\n",
    "def normalize_text_az(s: str, numbers_to_token=True, keep_sentence_punct=False) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    # emoji map first\n",
    "    for emo, tag in EMO_MAP.items():\n",
    "        s = s.replace(emo, f\" {tag} \")\n",
    "    s = html.unescape(s)\n",
    "    s = HTML_TAG_RE.sub(\" \", s)\n",
    "    s = URL_RE.sub(\" URL \", s)\n",
    "    s = EMAIL_RE.sub(\" EMAIL \", s)\n",
    "    s = PHONE_RE.sub(\" PHONE \", s)\n",
    "    # Keep text, split camelCase\n",
    "    s = re.sub(r\"#([A-Za-z0-9_]+)\", lambda m: \" \" + re.sub('([a-z])([A-Z])', r'\\1 \\2', m.group(1)) + \" \",s)\n",
    "    s = USER_RE.sub(\" USER \", s)\n",
    "    s = lower_az(s)\n",
    "    s = MULTI_PUNCT.sub(r\"\\1\", s)\n",
    "    if numbers_to_token:\n",
    "        s = re.sub(r\"\\d+\", \" <NUM> \", s)\n",
    "    if keep_sentence_punct:\n",
    "        s = re.sub(r\"[^\\w\\s<>'…ôƒüƒ±√∂≈ü√º√ß∆èƒûIƒ∞√ñ≈û√ú√áxqXQ.!?]\", \" \", s)\n",
    "    else:\n",
    "        s = re.sub(r\"[^\\w\\s<>'…ôƒüƒ±√∂≈ü√º√ß∆èƒûIƒ∞√ñ≈û√ú√áxqXQ]\", \" \", s)\n",
    "    s = MULTI_SPACE.sub(\" \", s).strip()\n",
    "\n",
    "    toks = TOKEN_RE.findall(s)\n",
    "    norm = []\n",
    "    mark_neg = 0\n",
    "    for t in toks:\n",
    "        t = REPEAT_CHARS.sub(r\"\\1\\1\", t) # limit repeated chars to 2\n",
    "        t = SLANG_MAP.get(t, t) # slang map\n",
    "        if t in NEGATORS:\n",
    "            norm.append(t); mark_neg = 3; continue\n",
    "        if mark_neg > 0 and t not in {\"URL\",\"EMAIL\",\"PHONE\",\"USER\"}:\n",
    "            norm.append(t + \"_NEG\"); mark_neg -= 1\n",
    "        else:\n",
    "            norm.append(t)\n",
    "    norm = [t for t in norm if not (len(t) == 1 and t not in {\"o\",\"e\"})]\n",
    "    return \" \".join(norm).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2ce51e",
   "metadata": {},
   "source": [
    "## Domain awareness (news / social / reviews / general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92bdb69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NEWS_HINTS = re.compile(r\"\\b(apa|trend|azertac|reuters|bloomberg|dha|aa)\\b\", re.I)\n",
    "SOCIAL_HINTS = re.compile(r\"\\b(rt)\\b|@|#|(?:üòÇ|üòç|üòä|üëç|üëé|üò°|üôÇ)\")\n",
    "REV_HINTS = re.compile(r\"\\b(azn|manat|qiym…ôt|aldƒ±m|ulduz|√ßox yax≈üƒ±|√ßox pis)\\b\", re.I)\n",
    "\n",
    "PRICE_RE = re.compile(r\"\\b\\d+\\s*(azn|manat)\\b\", re.I)\n",
    "STARS_RE = re.compile(r\"\\b([1-5])\\s*ulduz\\b\", re.I)\n",
    "POS_RATE = re.compile(r\"\\b√ßox yax≈üƒ±\\b\")\n",
    "NEG_RATE = re.compile(r\"\\b√ßox pis\\b\")\n",
    "\n",
    "def detect_domain(text: str) -> str:\n",
    "    s = (text or \"\").lower()\n",
    "    if NEWS_HINTS.search(s): return \"news\"\n",
    "    if SOCIAL_HINTS.search(s): return \"social\"\n",
    "    if REV_HINTS.search(s): return \"reviews\"\n",
    "    return \"general\"\n",
    "\n",
    "def domain_specific_normalize(cleaned: str, domain: str) -> str:\n",
    "    if domain == \"reviews\":\n",
    "        s = PRICE_RE.sub(\" <PRICE> \", cleaned)\n",
    "        s = STARS_RE.sub(lambda m: f\" <STARS_{m.group(1)}> \", s)\n",
    "        s = POS_RATE.sub(\" <RATING_POS> \", s)\n",
    "        s = NEG_RATE.sub(\" <RATING_NEG> \", s)\n",
    "        return \" \".join(s.split())\n",
    "    return cleaned\n",
    "\n",
    "def add_domain_tag(line: str, domain: str) -> str:\n",
    "    return f\"dom{domain} \" + line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9fe5af",
   "metadata": {},
   "source": [
    "## Processing functions (two-column Excel creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4221ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_sentiment_value(v, scheme: str):\n",
    "    if scheme == \"binary\":\n",
    "        try:\n",
    "            return 1.0 if int(v) == 1 else 0.0\n",
    "        except Exception:\n",
    "            return None\n",
    "    s = str(v).strip().lower()\n",
    "    if s in {\"pos\", \"positive\", \"1\", \"m√ºsb…ôt\", \"good\", \"pozitiv\"}: return 1.0\n",
    "    if s in {\"neu\", \"neutral\", \"2\", \"neytral\"}: return 0.5\n",
    "    if s in {\"neg\", \"negative\", \"0\", \"m…ônfi\", \"bad\", \"neqativ\"}: return 0.0\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_file(in_path, text_col, label_col, scheme, out_two_col_path, remove_stopwords=False):\n",
    "    df = pd.read_excel(in_path)\n",
    "    for c in [\"Unnamed: 0\", \"index\"]:\n",
    "        if c in df.columns:\n",
    "            df = df.drop(columns=[c])\n",
    "    assert text_col in df.columns and label_col in df.columns, f\"Missing columns in {in_path}\"\n",
    "\n",
    "    # cleaning\n",
    "    df = df.dropna(subset=[text_col])\n",
    "    df = df[df[text_col].astype(str).str.strip().str.len() > 0]\n",
    "    df = df.drop_duplicates(subset=[text_col])\n",
    "\n",
    "    df[\"cleaned_text\"] = df[text_col].astype(str).apply(lambda s: normalize_text_az(s))\n",
    "    df[\"__domain__\"]   = df[text_col].astype(str).apply(detect_domain)\n",
    "    df[\"cleaned_text\"] = df.apply(lambda r:\n",
    "                                  domain_specific_normalize(r[\"cleaned_text\"], r[\"__domain__\"]), axis=1)\n",
    "\n",
    "    df = df.dropna(subset=[\"cleaned_text\"])\n",
    "    df = df[df[\"cleaned_text\"].astype(str).str.strip() != \"\"]\n",
    "\n",
    "    if remove_stopwords:\n",
    "        sw = set([\"v…ô\",\"il…ô\",\"amma\",\"ancaq\",\"lakin\",\"ya\",\"h…ôm\",\"ki\",\"bu\",\"bir\",\"o\",\"biz\",\"siz\",\"m…ôn\",\"s…ôn\",\n",
    "                  \"orada\",\"burada\",\"b√ºt√ºn\",\"h…ôr\",\"artƒ±q\",\"√ßox\",\"az\",\"…ôn\",\"d…ô\",\"da\",\"√º√ß√ºn\"])\n",
    "        for keep in [\"deyil\",\"yox\",\"he√ß\",\"q…ôtiyy…ôn\",\"yoxdur\"]:\n",
    "            sw.discard(keep)\n",
    "        df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(\n",
    "            lambda s: \" \".join([t for t in s.split() if t not in sw])\n",
    "        )\n",
    "\n",
    "    # sentiment mapping\n",
    "    df[\"sentiment_value\"] = df[label_col].apply(lambda v: map_sentiment_value(v, scheme))\n",
    "    df = df.dropna(subset=[\"sentiment_value\"])\n",
    "    df[\"sentiment_value\"] = df[\"sentiment_value\"].astype(float)\n",
    "\n",
    "    out_df = df[[\"cleaned_text\", \"sentiment_value\"]].reset_index(drop=True)\n",
    "    Path(out_two_col_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    out_df.to_excel(out_two_col_path, index=False)\n",
    "    print(f\"Saved: {out_two_col_path} (rows={len(out_df)})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5650c96a",
   "metadata": {},
   "source": [
    "## Configure your dataset files here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76dfd53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs\\labeled-sentiment_output.xlsx (rows=2955)\n",
      "Saved: outputs\\test__1__output.xlsx (rows=4193)\n",
      "Saved: outputs\\train__3__output.xlsx (rows=19536)\n",
      "Saved: outputs\\train-00000-of-00001_output.xlsx (rows=41705)\n",
      "Saved: outputs\\merged_dataset_CSV__1__output.xlsx (rows=55662)\n",
      "\n",
      "labeled-sentiment.xlsx: orig=2958, removed_empty=1450, removed_dups=2, final=1506\n",
      "test__1_.xlsx: orig=4200, removed_empty=1991, removed_dups=1, final=2208\n",
      "train__3_.xlsx: orig=19600, removed_empty=9669, removed_dups=25, final=9906\n",
      "train-00000-of-00001.xlsx: orig=42000, removed_empty=20736, removed_dups=135, final=21129\n",
      "merged_dataset_CSV__1_.xlsx: orig=55673, removed_empty=27448, removed_dups=10, final=28215\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CFG = [\n",
    "    (\"labeled-sentiment.xlsx\", \"text\", \"sentiment\", \"tri\"),\n",
    "    (\"test__1_.xlsx\", \"text\", \"label\", \"binary\"),\n",
    "    (\"train__3_.xlsx\", \"text\", \"label\", \"binary\"),\n",
    "    (\"train-00000-of-00001.xlsx\", \"text\", \"labels\", \"tri\"),\n",
    "    (\"merged_dataset_CSV__1_.xlsx\", \"text\", \"labels\", \"binary\"),\n",
    "]\n",
    "\n",
    "# Run the processor to generate the required 2-column Excel outputs.\n",
    "for fname, tcol, lcol, scheme in CFG:\n",
    "    in_path = DATA_DIR / fname\n",
    "    out = OUT_DIR / f\"{Path(fname).stem}_output.xlsx\"\n",
    "    if not in_path.exists():\n",
    "        print(f\"[WARN] File not found: {in_path} ‚Äî skip if not used in your repo\")\n",
    "        continue\n",
    "    process_file(in_path, tcol, lcol, scheme, out, remove_stopwords=False)\n",
    "\n",
    "print()\n",
    "\n",
    "# Report on cleaning stats\n",
    "for fname, tcol, lcol, scheme in CFG:\n",
    "    p = DATA_DIR / fname\n",
    "    if not p.exists():\n",
    "        print(f\"Missing: {p}\")\n",
    "        continue\n",
    "    df = pd.read_excel(p)\n",
    "    orig = len(df)\n",
    "    non_null = df[df[tcol].notna() & df[tcol].astype(str).str.strip().str.len()>0]\n",
    "    after_dedup = non_null.drop_duplicates(subset=[tcol])\n",
    "    removed_empty = orig - len(non_null)\n",
    "    removed_dups  = len(non_null) - len(after_dedup)\n",
    "    print(f\"{p.name}: orig={orig}, removed_empty={removed_empty}, removed_dups={removed_dups}, final={len(after_dedup)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d907463c",
   "metadata": {},
   "source": [
    "## Build `corpus_all.txt` (domain-tagged, one sentence per line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ebb4c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote corpus_all.txt with 124353 lines\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_corpus_txt(input_files, text_cols, out_txt=\"corpus_all.txt\"):\n",
    "    lines = []\n",
    "    for (f, text_col) in zip(input_files, text_cols):\n",
    "        path = DATA_DIR / f\n",
    "        if not path.exists():\n",
    "            print(f\"[WARN] Missing for corpus: {path}\")\n",
    "            continue\n",
    "        df = pd.read_excel(path)\n",
    "        if text_col not in df.columns:\n",
    "            print(f\"[WARN] Column '{text_col}' missing in {path.name}\")\n",
    "            continue\n",
    "        for raw in df[text_col].dropna().astype(str):\n",
    "            dom = detect_domain(raw)\n",
    "            s = normalize_text_az(raw, keep_sentence_punct=True)\n",
    "            parts = re.split(r\"[.!?]+\", s)\n",
    "            for p in parts:\n",
    "                p = p.strip()\n",
    "                if not p: continue\n",
    "                p = re.sub(r\"[^\\w\\s…ôƒüƒ±√∂≈ü√º√ß∆èƒûIƒ∞√ñ≈û√ú√áxqXQ]\", \" \", p) # remove punctuation\n",
    "                p = \" \".join(p.split()).lower()\n",
    "                if p:\n",
    "                    lines.append(f\"dom{dom} \" + p)\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as w:\n",
    "        for ln in lines:\n",
    "            w.write(ln + \"\\n\")\n",
    "    print(f\"Wrote {out_txt} with {len(lines)} lines\")\n",
    "\n",
    "build_corpus_txt([c[0] for c in CFG], [c[1] for c in CFG], out_txt=\"corpus_all.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9129959",
   "metadata": {},
   "source": [
    "## Train Word2Vec & FastText embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b0c2e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 124051 sentences...\n",
      "Saved models to embeddings\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "two_col_files = [\n",
    "    f\"{Path(c[0]).stem}_output.xlsx\" for c in CFG\n",
    "]\n",
    "\n",
    "sentences = []\n",
    "for f in two_col_files:\n",
    "    p = OUT_DIR / f\n",
    "    if not p.exists():\n",
    "        print(f\"[WARN] 2-col file missing for embeddings: {p}\")\n",
    "        continue\n",
    "    df = pd.read_excel(p, usecols=[\"cleaned_text\"])\n",
    "    sentences.extend(df[\"cleaned_text\"].astype(str).str.split().tolist())\n",
    "\n",
    "print(f\"Training on {len(sentences)} sentences...\")\n",
    "EMB_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "w2v = Word2Vec(sentences=sentences, vector_size=300, window=5, min_count=3, sg=1, negative=10, epochs=10)\n",
    "w2v.save(str(EMB_DIR / \"word2vec.model\"))\n",
    "ft = FastText(sentences=sentences, vector_size=300, window=5, min_count=3, sg=1, min_n=3, max_n=6, epochs=10)\n",
    "ft.save(str(EMB_DIR / \"fasttext.model\"))\n",
    "print(\"Saved models to\", EMB_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9699ecb1",
   "metadata": {},
   "source": [
    "## Compare Word2Vec vs FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efc020bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical coverage (per dataset)\n",
      "labeled-sentiment_output.xlsx: W2V=0.932, FT(vocab)=0.932\n",
      "test__1__output.xlsx: W2V=0.987, FT(vocab)=0.987\n",
      "train__3__output.xlsx: W2V=0.990, FT(vocab)=0.990\n",
      "train-00000-of-00001_output.xlsx: W2V=0.943, FT(vocab)=0.943\n",
      "merged_dataset_CSV__1__output.xlsx: W2V=0.949, FT(vocab)=0.949\n",
      "\n",
      "Similarity (higher better for synonyms; lower better for antonyms)\n",
      "Synonyms: W2V=0.369, FT=0.444\n",
      "Antonyms: W2V=0.347, FT=0.424\n",
      "Separation (Syn - Ant): W2V=0.022, FT=0.020\n",
      "\n",
      "Nearest neighbors (qualitative)\n",
      "W2V NN for 'yax≈üƒ±': ['<RATING_POS>', 'yaxshi', 'iyi', 'yaxwi', 'awsome']\n",
      "FT NN for 'yax≈üƒ±': ['yax≈üƒ±ƒ±', 'yax≈üƒ±kƒ±', 'yax≈üƒ±ca', 'yax≈ü', 'yax≈üƒ±ya']\n",
      "W2V NN for 'pis': ['v…ôrdi≈ül…ôr…ô', '<RATING_NEG>', 'lire', 'g√ºnd', 's√ºr√ºkliyir']\n",
      "FT NN for 'pis': ['piis', 'pi', 'pis…ô', 'pixlr', 'pisle≈üdi']\n",
      "W2V NN for '√ßox': ['√ßoox', '√ß√∂x', 'b…ôy…ônilsin', '…ôladir', 'i≈ü√ßil…ôrind…ôn']\n",
      "FT NN for '√ßox': ['√ßox√ßox', '√ßoxx', '√ßoxh', '√ßo', '√ßoh']\n",
      "W2V NN for 'bahalƒ±': ['portretlerin…ô', 'radiusda', 'metallarla', 'yaxtalarƒ±', 'villalarƒ±']\n",
      "FT NN for 'bahalƒ±': ['bahalƒ±ƒ±', 'bahalƒ±sƒ±', 'bahalƒ±q', 'baharlƒ±', 'pahalƒ±']\n",
      "W2V NN for 'ucuz': ['≈üeytanbazardan', 'd√ºz…ôltdirilib', 'sududu', 'yelenaya', 'yamayƒ±rlar']\n",
      "FT NN for 'ucuz': ['ucuzu', 'ucuza', 'ucuzdu', 'ucuzluƒüa', 'ucuzdur']\n",
      "W2V NN for 'm√ºk…ômm…ôl': ['m√∂ht…ô≈ü…ômm', 'k…ôlim…ôyl…ô', 's√ºjetli', 'muk…ômm…ôl', 's√ºper']\n",
      "FT NN for 'm√ºk…ômm…ôl': ['m√ºk…ômm…ôll', 'm√ºk…ôm…ôl', 'muk…ômm…ôl', 'm√ºk…ômm…ôldi', 'm√ºk…ômm…ôlsiz']\n",
      "W2V NN for 'd…ôh≈ü…ôt': ['xal√ßalardan', 'ayranlarƒ±', 't…ôsirlidi', 'birda', 'onag√∂r…ô']\n",
      "FT NN for 'd…ôh≈ü…ôt': ['d…ôh≈ü…ôtd√º', 'd…ôh≈ü…ôt…ô', 'd…ôh≈ü…ôtizm', 'd…ôh≈ü…ôti', 'd…ôh≈ü…ôtdi']\n",
      "W2V NN for '<PRICE>': []\n",
      "FT NN for '<PRICE>': ['reeceep', 'recebzade', 'hurwitz', 'flight', 'lifcikle']\n",
      "W2V NN for '<RATING_POS>': ['deneyin', 's√ºper', 'yaradƒ±', '√ßook', '√∂yr…ôdici']\n",
      "FT NN for '<RATING_POS>': ['<RATING_NEG>', 's√ºperr', '√ßookk', '√ßokk', 's√ºper']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "w2v = Word2Vec.load(str(EMB_DIR / \"word2vec.model\"))\n",
    "ft = FastText.load(str(EMB_DIR / \"fasttext.model\"))\n",
    "\n",
    "seed_words = [\"yax≈üƒ±\",\"pis\",\"√ßox\",\"bahalƒ±\",\"ucuz\",\"m√ºk…ômm…ôl\",\"d…ôh≈ü…ôt\",\"<PRICE>\",\"<RATING_POS>\"]\n",
    "syn_pairs = [(\"yax≈üƒ±\",\"…ôla\"), (\"bahalƒ±\",\"qiym…ôtli\"), (\"ucuz\",\"s…ôrf…ôli\")]\n",
    "ant_pairs = [(\"yax≈üƒ±\",\"pis\"), (\"bahalƒ±\",\"ucuz\")]\n",
    "\n",
    "def lexical_coverage(model, tokens):\n",
    "    vocab = model.wv.key_to_index\n",
    "    return sum(1 for t in tokens if t in vocab) / max(1,len(tokens))\n",
    "\n",
    "def read_tokens(f):\n",
    "    df = pd.read_excel(f, usecols=[\"cleaned_text\"])\n",
    "    return [t for row in df[\"cleaned_text\"].astype(str) for t in row.split()]\n",
    "\n",
    "print(\"Lexical coverage (per dataset)\")\n",
    "for f in two_col_files:\n",
    "    p = OUT_DIR / f\n",
    "    if not p.exists():\n",
    "        print(f\"[WARN] Skipping coverage for missing: {p}\")\n",
    "        continue\n",
    "    toks = read_tokens(p)\n",
    "    cov_w2v = lexical_coverage(w2v, toks)\n",
    "    cov_ftv = lexical_coverage(ft, toks)  # FT vocab-based; FT can still embed OOV via subwords at inference\n",
    "    print(f\"{p.name}: W2V={cov_w2v:.3f}, FT(vocab)={cov_ftv:.3f}\")\n",
    "\n",
    "def pair_sim(model, pairs):\n",
    "    vals = []\n",
    "    for a,b in pairs:\n",
    "        try:\n",
    "            vals.append(model.wv.similarity(a,b))\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return sum(vals)/len(vals) if vals else float('nan')\n",
    "\n",
    "syn_w2v = pair_sim(w2v, syn_pairs)\n",
    "syn_ft = pair_sim(ft,  syn_pairs)\n",
    "ant_w2v = pair_sim(w2v, ant_pairs)\n",
    "ant_ft = pair_sim(ft,  ant_pairs)\n",
    "\n",
    "print(\"\\nSimilarity (higher better for synonyms; lower better for antonyms)\")\n",
    "print(f\"Synonyms: W2V={syn_w2v:.3f}, FT={syn_ft:.3f}\")\n",
    "print(f\"Antonyms: W2V={ant_w2v:.3f}, FT={ant_ft:.3f}\")\n",
    "print(f\"Separation (Syn - Ant): W2V={(syn_w2v - ant_w2v):.3f}, FT={(syn_ft - ant_ft):.3f}\")\n",
    "\n",
    "def neighbors(model, word, k=5):\n",
    "    try: return [w for w,_ in model.wv.most_similar(word, topn=k)]\n",
    "    except KeyError: return []\n",
    "\n",
    "print(\"\\nNearest neighbors (qualitative)\")\n",
    "for w in seed_words:\n",
    "    print(f\"W2V NN for '{w}':\", neighbors(w2v, w))\n",
    "    print(f\"FT NN for '{w}':\", neighbors(ft,  w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba077b2",
   "metadata": {},
   "source": [
    "## Mini-challenges utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b4c39bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, ['√ßox', 'yax≈üƒ±', 'salam', 'tamam', 'saƒüol', '√ßox'])\n",
      "\n",
      "Good Morning Everyone\n",
      "\n",
      "coxx gozel kino idi emo pos emo pos super film url\n"
     ]
    }
   ],
   "source": [
    "def count_deasciify(tokens):\n",
    "    changed = 0\n",
    "    out = []\n",
    "    for t in tokens:\n",
    "        tt = SLANG_MAP.get(t, t)\n",
    "        if tt != t: changed += 1\n",
    "        out.append(tt)\n",
    "    return changed, out\n",
    "\n",
    "def simple_hashtag_split(s: str):\n",
    "    return re.sub(r\"#([A-Za-z0-9_]+)\", lambda m: re.sub('([a-z])([A-Z])', r'\\1 \\2', m.group(1)), s)\n",
    "\n",
    "print(count_deasciify('cox yaxsi slm tmm sagol cox'.split()))\n",
    "print()\n",
    "print(simple_hashtag_split('#GoodMorningEveryone'))\n",
    "print()\n",
    "print(normalize_text_az(\"Coxxx gozel kino idi üëçüëç #SuperFilm http://example.com\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
